{
  "id": "core_concept_2",
  "title": "Multi-Head Attention: Capturing Diverse Relationships",
  "duration": 135,
  "narration": "Now, let's add another layer of sophistication: *Multi-Head Attention*.  Instead of performing attention once, we perform it multiple times in parallel, using different learned linear projections of the Queries, Keys, and Values.  Each 'head' learns to focus on different aspects of the input sequence.  Imagine you're analyzing a sentence and looking for different types of relationships: grammatical relationships, semantic relationships, and contextual relationships.  Each head can specialize in one of these types of relationships.  The outputs of all the heads are then concatenated and linearly transformed to produce the final output.  This allows the model to capture a richer and more nuanced understanding of the input sequence.  Multi-Head Attention is crucial for capturing complex dependencies and improving the overall performance of the Transformer.  It's like having multiple experts analyzing the same information from different perspectives.",
  "visual_description": {
    "VISUAL LAYOUT": "üé¨ SCENE: Multi-Head Attention: Capturing Diverse Relationships\n‚è±Ô∏è DURATION: 135 seconds\nüìä COMPLEXITY: Intermediate",
    "MAIN CONCEPTS TO VISUALIZE": "‚Ä¢ Multiple attention heads\n‚Ä¢ Linear projections\n‚Ä¢ Concatenation and transformation",
    "MATHEMATICAL FORMULAS": "MultiHead(Q, K, V) = Concat(head‚ÇÅ, ..., headh)WO",
    "VISUAL ELEMENTS TO CREATE": "Diagram showing multiple attention heads processing the input sequence in parallel.  Illustration of linear projections transforming Queries, Keys, and Values.  Visualization of the concatenation and transformation process.",
    "COLOR CODING SCHEME": "üü† Orange: Multi-Head Attention components\nüü£ Purple: Mathematical formulas",
    "COMPARISON TABLES": "Single-Head vs Multi-Head Attention: Limited vs Diverse Relationships"
  },
  "manim_code": "from manim import *\n\nclass MultiHeadAttentionScene(Scene):\n    def construct(self):\n        # Color scheme\n        orange = ORANGE\n        purple = PURPLE\n\n        # --- Opening Title Slide (10 seconds) ---\n        title = Text(\"Multi-Head Attention: Capturing Diverse Relationships\", font_size=36)\n        title.scale_to_fit_width(10)\n        self.play(Write(title))\n        self.wait(5)\n        context = Text(\"Understanding the core mechanism behind modern NLP models\", font_size=28)\n        context.scale_to_fit_width(10)\n        context.next_to(title, DOWN)\n        self.play(Write(context))\n        self.wait(5)\n        self.play(FadeOut(title, context))\n\n        # --- Single-Head Attention Introduction (20 seconds) ---\n        single_head_title = Text(\"Single-Head Attention: A Quick Recap\", font_size=32)\n        single_head_title.scale_to_fit_width(10)\n        self.play(Write(single_head_title))\n        self.wait(2)\n\n        # Simplified diagram of single-head attention\n        query = Text(\"Query (Q)\", font_size=24)\n        key = Text(\"Key (K)\", font_size=24)\n        value = Text(\"Value (V)\", font_size=24)\n        query.arrange(DOWN, aligned_edge=LEFT)\n        key.arrange(DOWN, aligned_edge=LEFT)\n        value.arrange(DOWN, aligned_edge=LEFT)\n        query.next_to(single_head_title, DOWN, buff=0.5)\n\n        attention_box = SurroundingRectangle(query, key, value, color=BLUE, buff=0.2)\n        attention_output = Text(\"Attention Output\", font_size=24)\n        attention_output.next_to(attention_box, RIGHT)\n\n        self.play(Create(attention_box), Write(attention_output))\n        self.wait(5)\n        self.play(Indicate(query, color=YELLOW, scale_factor=1.2), Indicate(key, color=YELLOW, scale_factor=1.2), Indicate(value, color=YELLOW, scale_factor=1.2))\n        self.wait(5)\n        self.play(FadeOut(single_head_title, query, key, value, attention_box, attention_output))\n\n        # --- The Limitation of Single-Head Attention (15 seconds) ---\n        limitation_title = Text(\"The Problem: Limited Relationship Capture\", font_size=32)\n        limitation_title.scale_to_fit_width(10)\n        self.play(Write(limitation_title))\n        self.wait(3)\n\n        sentence = Text(\"The cat sat on the mat.\", font_size=28)\n        sentence.scale_to_fit_width(10)\n        sentence.next_to(limitation_title, DOWN)\n        self.play(Write(sentence))\n        self.wait(5)\n\n        explanation = Text(\"Single-head attention might focus only on 'cat' and 'sat', missing the 'mat' relationship.\", font_size=24)\n        explanation.scale_to_fit_width(10)\n        explanation.next_to(sentence, DOWN)\n        self.play(Write(explanation))\n        self.wait(5)\n        self.play(FadeOut(limitation_title, sentence, explanation))\n\n        # --- Introducing Multi-Head Attention (20 seconds) ---\n        multi_head_title = Text(\"Multi-Head Attention: Capturing Diverse Relationships\", font_size=32)\n        multi_head_title.scale_to_fit_width(10)\n        self.play(Write(multi_head_title))\n        self.wait(2)\n\n        # Visual metaphor: multiple lenses\n        lenses = VGroup(*[Circle(radius=0.5, color=orange) for _ in range(4)])\n        lenses.arrange(RIGHT, buff=0.3)\n        lenses.next_to(multi_head_title, DOWN)\n        self.play(Create(lenses))\n        self.wait(5)\n\n        explanation_lenses = Text(\"Each 'head' is like a different lens, focusing on different aspects of the input.\", font_size=24)\n        explanation_lenses.scale_to_fit_width(10)\n        explanation_lenses.next_to(lenses, DOWN)\n        self.play(Write(explanation_lenses))\n        self.wait(5)\n        self.play(FadeOut(multi_head_title, lenses, explanation_lenses))\n\n        # --- The Multi-Head Attention Process (40 seconds) ---\n        process_title = Text(\"How Multi-Head Attention Works\", font_size=32)\n        process_title.scale_to_fit_width(10)\n        self.play(Write(process_title))\n        self.wait(2)\n\n        # Input sequence\n        input_seq = Text(\"Input Sequence (X)\", font_size=28)\n        input_seq.next_to(process_title, DOWN)\n        self.play(Write(input_seq))\n        self.wait(2)\n\n        # Linear Projections\n        linear_proj_title = Text(\"1. Linear Projections\", font_size=24)\n        linear_proj_title.next_to(input_seq, DOWN)\n        self.play(Write(linear_proj_title))\n\n        q_proj = Text(\"Q = XW_Q\", font_size=20)\n        k_proj = Text(\"K = XW_K\", font_size=20)\n        v_proj = Text(\"V = XW_V\", font_size=20)\n        q_proj.next_to(linear_proj_title, DOWN, aligned_edge=LEFT)\n        k_proj.next_to(q_proj, RIGHT)\n        v_proj.next_to(k_proj, RIGHT)\n        self.play(Write(q_proj), Write(k_proj), Write(v_proj))\n        self.wait(5)\n\n        # Multiple Heads\n        heads_title = Text(\"2. Multiple Heads (Parallel Attention)\", font_size=24)\n        heads_title.next_to(v_proj, DOWN)\n        self.play(Write(heads_title))\n\n        head1 = Rectangle(color=orange, width=2, height=1)\n        head2 = Rectangle(color=orange, width=2, height=1)\n        head1.next_to(heads_title, DOWN, aligned_edge=LEFT)\n        head2.next_to(head1, RIGHT)\n        self.play(Create(head1), Create(head2))\n        self.wait(3)\n\n        # Concatenation and Transformation\n        concat_title = Text(\"3. Concatenation & Transformation\", font_size=24)\n        concat_title.next_to(head2, DOWN)\n        self.play(Write(concat_title))\n\n        concat_formula = MathTex(\"Concat(head_1, ..., head_h)W_O\", font_size=20)\n        concat_formula.next_to(concat_title, DOWN)\n        self.play(Write(concat_formula))\n        self.wait(5)\n\n        self.play(FadeOut(process_title, input_seq, linear_proj_title, q_proj, k_proj, v_proj, heads_title, head1, head2, concat_title, concat_formula))\n\n        # --- Comparison Table (20 seconds) ---\n        comparison_title = Text(\"Single-Head vs. Multi-Head Attention\", font_size=32)\n        comparison_title.scale_to_fit_width(10)\n        self.play(Write(comparison_title))\n        self.wait(2)\n\n        table_data = [\n            [\"Feature\", \"Single-Head\", \"Multi-Head\"],\n            [\"Relationship Capture\", \"Limited\", \"Diverse\"],\n            [\"Parallel Processing\", \"No\", \"Yes\"],\n            [\"Complexity\", \"Lower\", \"Higher\"]\n        ]\n\n        table = Table(table_data, include_header=True, row_labels=None, col_labels=None, font_size=20)\n        table.scale_to_fit_width(10)\n        table.next_to(comparison_title, DOWN)\n        self.play(Create(table))\n        self.wait(10)\n        self.play(FadeOut(comparison_title, table))\n\n        # --- Conclusion (10 seconds) ---\n        conclusion_title = Text(\"Multi-Head Attention: A Powerful Tool\", font_size=32)\n        conclusion_title.scale_to_fit_width(10)\n        self.play(Write(conclusion_title))\n        self.wait(5)\n        summary = Text(\"Enables models to understand complex relationships within data.\", font_size=24)\n        summary.scale_to_fit_width(10)\n        summary.next_to(conclusion_title, DOWN)\n        self.play(Write(summary))\n        self.wait(5)\n        self.play(FadeOut(conclusion_title, summary))"
}