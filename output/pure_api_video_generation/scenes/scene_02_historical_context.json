{
  "id": "historical_context",
  "title": "Before Transformers: The Reign of Recurrence and Convolution",
  "duration": 105,
  "narration": "Let's rewind a bit. For years, sequence transduction ‚Äì the process of converting one sequence into another, like translating English to German ‚Äì was dominated by recurrent neural networks (RNNs).  RNNs, and their more sophisticated variants like LSTMs and GRUs, processed data sequentially. Imagine a conveyor belt where each item (word) is processed one at a time.  They had a 'memory' to retain information about previous items.  Convolutional Neural Networks (CNNs) were also used, especially for tasks like image recognition, but adapted for sequences, they process chunks of the sequence at a time.  However, both had limitations. RNNs struggled with long-range dependencies ‚Äì remembering information from the beginning of a long sentence. CNNs, while parallelizable, required multiple layers to capture relationships between distant words.  The 'attention mechanism' was introduced *alongside* RNNs to help them focus on relevant parts of the input sequence, but it was still an add-on, not the core architecture.  The problem wasn't just speed; it was the inherent sequential nature of RNNs that limited parallelization and scalability.  This meant training these models on massive datasets was incredibly time-consuming and resource-intensive.",
  "visual_description": {
    "VISUAL LAYOUT": "üé¨ SCENE: Before Transformers: The Reign of Recurrence and Convolution\n‚è±Ô∏è DURATION: 105 seconds\nüìä COMPLEXITY: Beginner",
    "MAIN CONCEPTS TO VISUALIZE": "‚Ä¢ RNNs, LSTMs, GRUs\n‚Ä¢ CNNs for sequence processing\n‚Ä¢ The role of attention as an add-on",
    "MATHEMATICAL FORMULAS": "None at this stage",
    "VISUAL ELEMENTS TO CREATE": "Animated diagrams of RNNs processing a sequence sequentially.  Visual representation of the 'vanishing gradient' problem in RNNs.  Comparison of RNN and CNN processing methods.  Illustration of attention highlighting relevant words.",
    "COLOR CODING SCHEME": "üîµ Blue: RNNs and CNNs\nüü† Orange: Attention mechanism",
    "COMPARISON TABLES": "RNN vs CNN: Sequential vs Parallel"
  },
  "manim_code": "from manim import *\n\nclass BeforeTransformers(Scene):\n    def construct(self):\n        # --- Title Slide (5 seconds) ---\n        title = Text(\"Before Transformers: RNNs, CNNs & Attention\", font_size=36)\n        title.scale_to_fit_width(12)\n        subtitle = Text(\"Foundations for Modern Sequence Modeling\", font_size=28)\n        subtitle.scale_to_fit_width(12)\n        self.play(Write(title))\n        self.wait(3)\n        self.play(Write(subtitle))\n        self.wait(2)\n        self.play(FadeOut(title, subtitle))\n\n        # --- RNN Introduction (25 seconds) ---\n        rnn_title = Text(\"Recurrent Neural Networks (RNNs)\", font_size=32)\n        rnn_title.scale_to_fit_width(12)\n        self.play(Write(rnn_title))\n        self.wait(2)\n\n        # Simple RNN diagram\n        rect1 = Rectangle(color=BLUE, width=1, height=1)\n        rect2 = Rectangle(color=BLUE, width=1, height=1).next_to(rect1, RIGHT)\n        rect3 = Rectangle(color=BLUE, width=1, height=1).next_to(rect2, RIGHT)\n\n        arrow1 = Arrow(rect1.get_right(), rect2.get_left())\n        arrow2 = Arrow(rect2.get_right(), rect3.get_left())\n\n        self.play(Create(rect1), Create(arrow1))\n        self.wait(2)\n        self.play(Create(rect2), Create(arrow2))\n        self.wait(2)\n        self.play(Create(rect3))\n        self.wait(3)\n\n        text1 = Text(\"Input Sequence\", font_size=24)\n        text1.scale_to_fit_width(8)\n        text1.next_to(rect1, DOWN)\n        self.play(Write(text1))\n\n        text2 = Text(\"Hidden State\", font_size=24)\n        text2.scale_to_fit_width(8)\n        text2.next_to(rect1, UP)\n        self.play(Write(text2))\n\n        self.wait(5)\n        self.play(FadeOut(rnn_title, rect1, rect2, rect3, arrow1, arrow2, text1, text2))\n\n        # --- Vanishing Gradient Problem (20 seconds) ---\n        vg_title = Text(\"The Vanishing Gradient Problem\", font_size=32)\n        vg_title.scale_to_fit_width(12)\n        self.play(Write(vg_title))\n        self.wait(2)\n\n        # Visual metaphor: a long, winding path\n        path = VMobject(color=RED)\n        path.set_points_as_corners([LEFT * 5, LEFT * 4 + DOWN, LEFT * 3 + UP, LEFT * 2 + DOWN, LEFT * 1 + UP, ORIGIN + DOWN, RIGHT + UP])\n\n        gradient_arrow = Arrow(LEFT * 5, RIGHT + UP, buff=0.1, color=GREEN)\n        gradient_arrow.add_tip(Tip(length=0.2))\n\n        self.play(Create(path), Create(gradient_arrow))\n        self.wait(5)\n\n        text3 = Text(\"Gradient weakens as it travels back\", font_size=24)\n        text3.scale_to_fit_width(10)\n        text3.next_to(path, DOWN)\n        self.play(Write(text3))\n        self.wait(8)\n        self.play(FadeOut(vg_title, path, gradient_arrow, text3))\n\n        # --- LSTM/GRU Introduction (20 seconds) ---\n        lstm_title = Text(\"LSTMs & GRUs: Solving the Problem\", font_size=32)\n        lstm_title.scale_to_fit_width(12)\n        self.play(Write(lstm_title))\n        self.wait(2)\n\n        # Simplified LSTM cell diagram\n        lstm_cell = Rectangle(color=BLUE, width=2, height=1.5)\n        gate_text = Text(\"Gates control information flow\", font_size=24)\n        gate_text.scale_to_fit_width(10)\n        gate_text.next_to(lstm_cell, DOWN)\n\n        self.play(Create(lstm_cell), Write(gate_text))\n        self.wait(8)\n\n        text4 = Text(\"More complex memory cells\", font_size=24)\n        text4.scale_to_fit_width(10)\n        text4.next_to(lstm_cell, UP)\n        self.play(Write(text4))\n        self.wait(5)\n        self.play(FadeOut(lstm_title, lstm_cell, gate_text, text4))\n\n        # --- CNNs for Sequence Processing (20 seconds) ---\n        cnn_title = Text(\"Convolutional Neural Networks (CNNs) for Sequences\", font_size=32)\n        cnn_title.scale_to_fit_width(12)\n        self.play(Write(cnn_title))\n        self.wait(2)\n\n        # CNN diagram\n        conv_layer = Rectangle(color=BLUE, width=4, height=1)\n        input_seq = Text(\"Input Sequence\", font_size=24)\n        input_seq.scale_to_fit_width(10)\n        input_seq.next_to(conv_layer, DOWN)\n\n        self.play(Create(conv_layer), Write(input_seq))\n        self.wait(5)\n\n        text5 = Text(\"Parallel processing of local patterns\", font_size=24)\n        text5.scale_to_fit_width(10)\n        text5.next_to(conv_layer, UP)\n        self.play(Write(text5))\n        self.wait(8)\n        self.play(FadeOut(cnn_title, conv_layer, input_seq, text5))\n\n        # --- Attention Mechanism (15 seconds) ---\n        attention_title = Text(\"Attention: Focusing on What Matters\", font_size=32)\n        attention_title.scale_to_fit_width(12)\n        self.play(Write(attention_title))\n        self.wait(2)\n\n        # Attention highlighting\n        sentence = Text(\"The quick brown fox jumps over the lazy dog.\", font_size=24)\n        sentence.scale_to_fit_width(12)\n        highlight = SurroundingRectangle(sentence[10:13], color=ORANGE, buff=0.1) # Highlight \"fox\"\n\n        self.play(Write(sentence), Create(highlight))\n        self.wait(5)\n\n        text6 = Text(\"Attention weights highlight relevant words\", font_size=24)\n        text6.scale_to_fit_width(10)\n        text6.next_to(sentence, DOWN)\n        self.play(Write(text6))\n        self.wait(3)\n        self.play(FadeOut(attention_title, sentence, highlight, text6))"
}