{
  "id": "core_concept_1",
  "title": "Scaled Dot-Product Attention: The Heart of the Transformer",
  "duration": 140,
  "narration": "Let's dive into the core of the Transformer: *Scaled Dot-Product Attention*. This is how the model calculates how much attention each word should pay to every other word.  It involves three key components: *Queries*, *Keys*, and *Values*.  Think of it like a search engine.  The *Query* is what you're searching for. The *Keys* are the keywords associated with each document.  The *Values* are the actual content of the documents.  The attention mechanism calculates a score for each Key based on its similarity to the Query.  This score determines how much of the corresponding Value is used to generate the output.  Mathematically, this is done by taking the dot product of the Query and each Key, scaling it down (to prevent gradients from exploding), and then applying a softmax function to normalize the scores into probabilities.  These probabilities represent the attention weights.  Higher weights mean more attention.  This process is repeated for each word in the input sequence, resulting in a weighted sum of the Values, which represents the attention-weighted context for that word.",
  "visual_description": {
    "VISUAL LAYOUT": "üé¨ SCENE: Scaled Dot-Product Attention: The Heart of the Transformer\n‚è±Ô∏è DURATION: 140 seconds\nüìä COMPLEXITY: Intermediate",
    "MAIN CONCEPTS TO VISUALIZE": "‚Ä¢ Queries, Keys, and Values\n‚Ä¢ Dot product calculation\n‚Ä¢ Softmax function\n‚Ä¢ Attention weights",
    "MATHEMATICAL FORMULAS": "Attention(Q, K, V) = softmax(QK·µÄ / ‚àödk)V",
    "VISUAL ELEMENTS TO CREATE": "Visual metaphor of a search engine with Queries, Keys, and Values.  Animated diagram showing the dot product calculation.  Illustration of the softmax function normalizing scores.  Visualization of attention weights as connections between words.",
    "COLOR CODING SCHEME": "üü£ Purple: Mathematical formulas\nüü† Orange: Attention mechanism components",
    "COMPARISON TABLES": "None at this stage"
  },
  "manim_code": "from manim import *\n\nclass ScaledDotProductAttention(Scene):\n    def construct(self):\n        # --- Introduction (10 seconds) ---\n        title = Text(\"Scaled Dot-Product Attention\", font_size=48)\n        title.scale_to_fit_width(10)\n        subtitle = Text(\"The Heart of the Transformer\", font_size=32)\n        subtitle.scale_to_fit_width(10)\n        self.play(Write(title))\n        self.wait(3)\n        self.play(Write(subtitle))\n        self.wait(7)\n\n        # --- Queries, Keys, and Values (30 seconds) ---\n        self.clear()\n        search_engine = SVGMobject(\"search_engine.svg\", width=4, height=3) # Replace with actual SVG path or file\n        search_engine.move_to(ORIGIN)\n        self.play(Create(search_engine))\n        self.wait(2)\n\n        query_text = Text(\"Query: What are cats?\", font_size=28)\n        query_text.scale_to_fit_width(8)\n        query_text.next_to(search_engine, UP, buff=0.5)\n        self.play(Write(query_text))\n        self.wait(3)\n\n        key_text = Text(\"Keys: Documents about animals\", font_size=28)\n        key_text.scale_to_fit_width(8)\n        key_text.next_to(query_text, DOWN, aligned_edge=LEFT, buff=0.5)\n        self.play(Write(key_text))\n        self.wait(3)\n\n        value_text = Text(\"Values: Content of those documents\", font_size=28)\n        value_text.scale_to_fit_width(8)\n        value_text.next_to(key_text, DOWN, aligned_edge=LEFT, buff=0.5)\n        self.play(Write(value_text))\n        self.wait(7)\n\n        self.play(Indicate(query_text, color=ORANGE), Indicate(key_text, color=ORANGE), Indicate(value_text, color=ORANGE))\n        self.wait(10)\n\n        # --- Dot Product Calculation (40 seconds) ---\n        self.clear()\n        q = Tex(\"Q = Query\", font_size=32)\n        k = Tex(\"K = Key\", font_size=32)\n        v = Tex(\"V = Value\", font_size=32)\n        q.to_edge(UP)\n        k.next_to(q, DOWN, aligned_edge=LEFT)\n        v.next_to(k, DOWN, aligned_edge=LEFT)\n\n        self.play(Write(q), Write(k), Write(v))\n        self.wait(2)\n\n        dot_product = Tex(\"Q \\\\cdot K^T\", font_size=32)\n        dot_product.next_to(v, DOWN, buff=1)\n        self.play(Write(dot_product))\n        self.wait(5)\n\n        explanation_dot = Text(\"Measures similarity between Query and Keys\", font_size=24)\n        explanation_dot.scale_to_fit_width(8)\n        explanation_dot.next_to(dot_product, DOWN, buff=0.5)\n        self.play(Write(explanation_dot))\n        self.wait(10)\n\n        scaled_dot_product = Tex(\"Q \\\\cdot K^T / \\\\sqrt{d_k}\", font_size=32)\n        scaled_dot_product.next_to(explanation_dot, DOWN, buff=1)\n        self.play(Write(scaled_dot_product))\n        self.wait(5)\n\n        explanation_scale = Text(\"Scaling prevents gradients from vanishing\", font_size=24)\n        explanation_scale.scale_to_fit_width(8)\n        explanation_scale.next_to(scaled_dot_product, DOWN, buff=0.5)\n        self.play(Write(explanation_scale))\n        self.wait(10)\n\n        # --- Softmax and Attention Weights (40 seconds) ---\n        self.clear()\n        softmax_formula = Tex(\"softmax(\\\\frac{Q \\\\cdot K^T}{\\\\sqrt{d_k}})\", font_size=32)\n        softmax_formula.to_edge(UP)\n        self.play(Write(softmax_formula))\n        self.wait(3)\n\n        explanation_softmax = Text(\"Normalizes scores into probabilities\", font_size=24)\n        explanation_softmax.scale_to_fit_width(8)\n        explanation_softmax.next_to(softmax_formula, DOWN, buff=0.5)\n        self.play(Write(explanation_softmax))\n        self.wait(7)\n\n        attention_weights = Tex(\"Attention Weights\", font_size=32)\n        attention_weights.next_to(explanation_softmax, DOWN, buff=1)\n        self.play(Write(attention_weights))\n        self.wait(5)\n\n        attention_equation = Tex(\"Attention(Q, K, V) = softmax(\\\\frac{Q \\\\cdot K^T}{\\\\sqrt{d_k}})V\", font_size=32)\n        attention_equation.next_to(attention_weights, DOWN, buff=1)\n        attention_equation.set_color(PURPLE)\n        self.play(Write(attention_equation))\n        self.wait(10)\n\n        explanation_attention = Text(\"Weighted sum of Values based on attention weights\", font_size=24)\n        explanation_attention.scale_to_fit_width(8)\n        explanation_attention.next_to(attention_equation, DOWN, buff=0.5)\n        self.play(Write(explanation_attention))\n        self.wait(10)\n\n        # --- Summary (20 seconds) ---\n        self.clear()\n        summary_title = Text(\"In Summary\", font_size=48)\n        summary_title.scale_to_fit_width(10)\n        self.play(Write(summary_title))\n        self.wait(3)\n\n        summary_points = VGroup(\n            Text(\"Attention focuses on relevant parts of the input.\", font_size=28),\n            Text(\"Queries, Keys, and Values are core components.\", font_size=28),\n            Text(\"Softmax normalizes scores for probability distribution.\", font_size=28)\n        )\n        summary_points.arrange(DOWN, aligned_edge=LEFT)\n        summary_points.scale_to_fit_width(10)\n        self.play(Write(summary_points))\n        self.wait(17)"
}