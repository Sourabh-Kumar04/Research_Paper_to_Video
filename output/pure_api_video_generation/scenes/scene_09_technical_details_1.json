{
  "id": "technical_details_1",
  "title": "Positional Encoding: Adding Order to the Chaos",
  "duration": 120,
  "narration": "One crucial detail: the Transformer doesn't inherently understand the order of words in a sequence because it processes them all simultaneously.  To address this, we use *positional encoding*.  This involves adding a vector to each word embedding that represents its position in the sequence.  These vectors are calculated using sine and cosine functions of different frequencies.  This allows the model to distinguish between words based on their position.  Imagine you have two identical words in a sentence.  Without positional encoding, the model wouldn't know which word comes first.  Positional encoding provides that information.  The positional encoding vectors are added to the word embeddings before they are fed into the attention mechanism.  This ensures that the model has access to both the semantic meaning of the words and their positional information.  Without positional encoding, the Transformer would be unable to effectively process sequential data.",
  "visual_description": {
    "VISUAL LAYOUT": "üé¨ SCENE: Positional Encoding: Adding Order to the Chaos\n‚è±Ô∏è DURATION: 120 seconds\nüìä COMPLEXITY: Intermediate",
    "MAIN CONCEPTS TO VISUALIZE": "‚Ä¢ The need for positional information\n‚Ä¢ Sine and cosine functions\n‚Ä¢ Adding positional encoding to word embeddings",
    "MATHEMATICAL FORMULAS": "PE(pos, 2i) = sin(pos / 10000^(2i/dmodel))\nPE(pos, 2i+1) = cos(pos / 10000^(2i/dmodel))",
    "VISUAL ELEMENTS TO CREATE": "Illustration of sine and cosine waves with different frequencies.  Visualization of positional encoding vectors being added to word embeddings.  Comparison of word embeddings with and without positional encoding.",
    "COLOR CODING SCHEME": "üîµ Blue: Positional encoding\nüü£ Purple: Mathematical formulas",
    "COMPARISON TABLES": "None at this stage"
  },
  "manim_code": "from manim import *\n\nclass PositionalEncodingScene(Scene):\n    def construct(self):\n        # --- Introduction (0-15 seconds) ---\n        title = Text(\"Positional Encoding: Adding Order to Chaos\", font_size=36)\n        title.scale_to_fit_width(10)\n        self.play(Write(title))\n        self.wait(5)\n\n        intro_text = Text(\"Word embeddings represent words as vectors, but they lose information about word order.\", font_size=28)\n        intro_text.scale_to_fit_width(10)\n        intro_text.next_to(title, DOWN, buff=0.5)\n        self.play(Write(intro_text))\n        self.wait(10)\n\n        # --- The Problem: Order Matters (15-30 seconds) ---\n        problem_title = Text(\"Why Order Matters\", font_size=32)\n        problem_title.scale_to_fit_width(10)\n        problem_title.to_edge(UP)\n        self.play(Transform(title, problem_title))\n\n        example_sentence1 = Text(\"The cat sat on the mat.\", font_size=28)\n        example_sentence1.scale_to_fit_width(10)\n        example_sentence1.next_to(problem_title, DOWN, buff=0.5)\n\n        example_sentence2 = Text(\"The mat sat on the cat.\", font_size=28)\n        example_sentence2.scale_to_fit_width(10)\n        example_sentence2.next_to(example_sentence1, DOWN, buff=0.5)\n\n        self.play(Write(example_sentence1))\n        self.wait(3)\n        self.play(Write(example_sentence2))\n        self.wait(5)\n\n        highlight_order = Indicate(example_sentence1[0:3], color=YELLOW, scale_factor=1.2)\n        highlight_order2 = Indicate(example_sentence2[0:3], color=YELLOW, scale_factor=1.2)\n        self.play(highlight_order, highlight_order2)\n        self.wait(5)\n\n        # --- Introducing Positional Encoding (30-45 seconds) ---\n        pe_title = Text(\"Introducing Positional Encoding\", font_size=32)\n        pe_title.scale_to_fit_width(10)\n        self.play(Transform(title, pe_title))\n\n        pe_explanation = Text(\"Positional encoding adds information about the position of each word in the sequence.\", font_size=28)\n        pe_explanation.scale_to_fit_width(10)\n        pe_explanation.next_to(pe_title, DOWN, buff=0.5)\n        self.play(Write(pe_explanation))\n        self.wait(10)\n\n        # --- Sine and Cosine Waves (45-60 seconds) ---\n        sine_cosine_title = Text(\"Sine and Cosine Waves\", font_size=32)\n        sine_cosine_title.scale_to_fit_width(10)\n        self.play(Transform(title, sine_cosine_title))\n\n        axes = Axes(\n            x_range=[0, 10, 1],\n            y_range=[-1.5, 1.5, 0.5],\n            x_length=6,\n            y_length=3,\n            axis_config={\"include_numbers\": False}\n        ).to_edge(DOWN)\n\n        sine_wave = axes.plot(lambda x: np.sin(x), color=BLUE)\n        cosine_wave = axes.plot(lambda x: np.cos(x), color=GREEN)\n\n        self.play(Create(axes), Create(sine_wave), Create(cosine_wave))\n        self.wait(5)\n\n        sine_label = Text(\"Sine Wave\", font_size=24, color=BLUE).next_to(sine_wave, UP)\n        cosine_label = Text(\"Cosine Wave\", font_size=24, color=GREEN).next_to(cosine_wave, UP)\n        self.play(Write(sine_label), Write(cosine_label))\n        self.wait(5)\n\n        # --- The Formula (60-75 seconds) ---\n        formula_title = Text(\"The Positional Encoding Formula\", font_size=32)\n        formula_title.scale_to_fit_width(10)\n        self.play(Transform(title, formula_title))\n\n        formula_pe = MathTex(\n            \"PE(pos, 2i) = sin(pos / 10000^{\\\\frac{2i}{d_{model}}})\",\n            \"PE(pos, 2i+1) = cos(pos / 10000^{\\\\frac{2i}{d_{model}}})\",\n            font_size=28\n        ).scale_to_fit_width(10)\n        formula_pe.next_to(formula_title, DOWN, buff=0.5)\n        self.play(Write(formula_pe))\n        self.wait(10)\n\n        formula_explanation = Text(\"pos = position, i = dimension\", font_size=24).next_to(formula_pe, DOWN, buff=0.5)\n        self.play(Write(formula_explanation))\n        self.wait(5)\n\n        # --- Adding PE to Word Embeddings (75-90 seconds) ---\n        embedding_title = Text(\"Adding Positional Encoding\", font_size=32)\n        embedding_title.scale_to_fit_width(10)\n        self.play(Transform(title, embedding_title))\n\n        word_embedding = Rectangle(color=ORANGE, width=2, height=1).to_edge(LEFT)\n        pe_vector = Rectangle(color=BLUE, width=2, height=1).next_to(word_embedding, RIGHT)\n\n        embedding_label = Text(\"Word Embedding\", font_size=24, color=ORANGE).next_to(word_embedding, DOWN)\n        pe_label = Text(\"Positional Encoding\", font_size=24, color=BLUE).next_to(pe_vector, DOWN)\n\n        self.play(Create(word_embedding), Create(pe_vector), Write(embedding_label), Write(pe_label))\n        self.wait(3)\n\n        combined_embedding = Rectangle(color=PURPLE, width=2, height=1).next_to(pe_vector, RIGHT)\n        arrow = Arrow(pe_vector.get_right(), combined_embedding.get_left())\n        addition_text = Text(\" + \", font_size=32).next_to(arrow, UP)\n\n        self.play(Create(arrow), Write(addition_text), Create(combined_embedding))\n        self.wait(5)\n\n        # --- Before and After (90-105 seconds) ---\n        comparison_title = Text(\"Before and After\", font_size=32)\n        comparison_title.scale_to_fit_width(10)\n        self.play(Transform(title, comparison_title))\n\n        before_embedding = Rectangle(color=ORANGE, width=2, height=1).to_edge(LEFT)\n        after_embedding = Rectangle(color=PURPLE, width=2, height=1).to_edge(RIGHT)\n\n        before_label = Text(\"Without Positional Encoding\", font_size=24, color=ORANGE).next_to(before_embedding, DOWN)\n        after_label = Text(\"With Positional Encoding\", font_size=24, color=PURPLE).next_to(after_embedding, DOWN)\n\n        self.play(Create(before_embedding), Create(after_embedding), Write(before_label), Write(after_label))\n        self.wait(5)\n\n        # --- Summary (105-120 seconds) ---\n        summary_title = Text(\"Summary\", font_size=32)\n        summary_title.scale_to_fit_width(10)\n        self.play(Transform(title, summary_title))\n\n        summary_text = Text(\"Positional encoding adds crucial information about word order to word embeddings, enabling models to understand sequence meaning.\", font_size=28)\n        summary_text.scale_to_fit_width(10)\n        summary_text.next_to(summary_title, DOWN, buff=0.5)\n        self.play(Write(summary_text))\n        self.wait(10)"
}